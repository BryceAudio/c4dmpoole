<h1>pubs2018.bib</h1><a name="benetos2018approachesanalysis"></a><pre>
@incollection{<a href="pubs2018_raw.html#benetos2018approachesanalysis">benetos2018approachesanalysis</a>,
  title = {Approaches to complex sound scene analysis},
  author = {BENETOS, E and STOWELL, D and PLUMBLEY, M},
  booktitle = {Computational Analysis of Sound Scenes and Events},
  publisher = {Springer International Publishing},
  year = {2018},
  edition = {1},
  editor = {Virtanen, T and PLUMBLEY, M and Ellis, D},
  month = {Jan},
  number = {8},
  pages = {215--242},
  series = {Signals \& Communication},
  day = {1},
  doi = {10.1007/978-3-319-63450-0},
  isbn = {978-3-319-63449-4},
  numberofpieces = {14},
  publicationstatus = {published},
  timestamp = {2018.02.05},
  url = {<a href="http://www.springer.com/gb/book/9783319634494">http://www.springer.com/gb/book/9783319634494</a>}
}
</pre>

<a name="panteli2018acorpora"></a><pre>
@article{<a href="pubs2018_raw.html#panteli2018acorpora">panteli2018acorpora</a>,
  title = {A review of manual and computational approaches for the study of world music corpora},
  author = {PANTELI, M and BENETOS, E and DIXON, S},
  journal = {Journal of New Music Research},
  year = {2018},
  month = {Jan},
  pages = {176--189},
  volume = {47},
  abstract = {The comparison of world music cultures has been a recurring topic in the field of musicology since the end of the nineteenth century. Recent advances in technology in the field of Music Information Retrieval allow for a large-scale analysis of music corpora. We review manual and computational approaches in the literature that fall within the scope of music corpus research and world music analysis. With a large-scale computational music corpus analysis in mind, we compare the tools and research questions addressed by each study and discuss strengths and weaknesses. Taking into account critical remarks from experts in the field and challenges involved in a large-scale computational analysis, we discuss how this line of research can be improved in future work.},
  day = {8},
  doi = {10.1080/09298215.2017.1418896},
  issn = {0929-8215},
  issue = {2},
  publicationstatus = {published},
  publisher = {Taylor \& Francis (Routledge)},
  timestamp = {2018.02.05}
}
</pre>

<a name="mesaros2018detectionchallenge"></a><pre>
@article{<a href="pubs2018_raw.html#mesaros2018detectionchallenge">mesaros2018detectionchallenge</a>,
  title = {Detection and Classification of Acoustic Scenes and Events: Outcome of the DCASE 2016 Challenge},
  author = {Mesaros, A and Heittola, T and Benetos, E and Foster, P and Lagrange, M and Virtanen, T and Plumbley, M},
  journal = {IEEE/ACM Transactions on Audio, Speech and Language Processing},
  year = {2018},
  month = {Feb},
  pages = {379--393},
  volume = {26},
  abstract = {Public evaluation campaigns and datasets promote active development in target research areas, allowing direct comparison of algorithms. The second edition of the challenge on detection and classification of acoustic scenes and events (DCASE 2016) has offered such an opportunity for development of the state-of-the-art methods, and succeeded in drawing together a large number of participants from academic and industrial backgrounds. In this paper, we report on the tasks and outcomes of the DCASE 2016 challenge. The challenge comprised four tasks: acoustic scene classification, sound event detection in synthetic audio, sound event detection in real-life audio, and domestic audio tagging. We present each task in detail and analyze the submitted systems in terms of design and performance. We observe the emergence of deep learning as the most popular classification method, replacing the traditional approaches based on Gaussian mixture models and support vector machines. By contrast, feature representations have not changed substantially throughout the years, as mel frequency-based representations predominate in all tasks. The datasets created for and used in DCASE 2016 are publicly available and are a valuable resource for further research.},
  day = {1},
  doi = {10.1109/TASLP.2017.2778423},
  issn = {2329-9304},
  issue = {2},
  publicationstatus = {published},
  publisher = {Institute of Electrical and Electronics Engineers},
  timestamp = {2018.02.05},
  url = {<a href="http://ieeexplore.ieee.org/document/8123864/">http://ieeexplore.ieee.org/document/8123864/</a>}
}
</pre>

<a name="choi2018thetagging"></a><pre>
@article{<a href="pubs2018_raw.html#choi2018thetagging">choi2018thetagging</a>,
  title = {The Effects of Noisy Labels on Deep Convolutional Neural Networks for Music Tagging},
  author = {Choi, K and Fazekas, G and Sandler, M and Cho, K},
  journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
  year = {2018},
  note = {date-added: 2017-12-21 19:00:24 +0000
date-modified: 2017-12-21 19:15:46 +0000
keywords: evaluation, music tagging, deep learning, CNN
bdsk-url-1: https://arxiv.org/pdf/1706.02361.pdf
bdsk-url-2: https://dx.doi.org/10.1109/TETCI.2017.2771298},
  number = {X (in press)},
  volume = {X},
  abstract = {Deep neural networks (DNN) have been successfully applied to music classification including music tagging. However, there are several open questions regarding the training, evaluation, and analysis of DNNs. In this article, we investigate specific aspects of neural networks, the effects of noisy labels, to deepen our understanding of their properties. We analyse and (re-)validate a large music tagging dataset to investigate the reliability of training and evaluation. Using a trained network, we compute label vector similarities which is compared to groundtruth similarity. The results highlight several important aspects of music tagging and neural networks. We show that networks can be effective despite relatively large error rates in groundtruth datasets, while conjecturing that label noise can be the cause of varying tag-wise performance differences. Lastly, the analysis of our trained network provides valuable insight into the relationships between music tags. These results highlight the benefit of using data-driven methods to address automatic music tagging.},
  doi = {10.1109/TETCI.2017.2771298},
  publicationstatus = {accepted},
  timestamp = {2018.02.05},
  url = {https://arxiv.org/pdf/1706.02361.pdf}
}
</pre>

<a name="skach2018embodiedarts"></a><pre>
@inproceedings{<a href="pubs2018_raw.html#skach2018embodiedarts">skach2018embodiedarts</a>,
  title = {Embodied Interactions with E-Textiles and the Internet of Sounds for Performing Arts},
  author = {SKACH, S and XAMBO, A and TURCHET, L and Stolfi, A and STEWART, RL and BARTHET, MHE},
  year = {2018},
  month = {Mar},
  organization = {Stockholm, Sweden},
  abstract = {This paper presents initial steps towards the design of an embedded system for body-centric sonic performance. The proposed prototyping system allows performers to manipulate sounds through gestural interactions captured by textile wearable sensors. The e-textile sensor data control, in real-time, audio synthesis algorithms working with content from Audio Commons, a novel web-based ecosystem for re-purposing crowd-sourced audio. The system enables creative embodied music interactions by combining seamless physical e-textiles with web-based digital audio technologies.},
  conference = {ACM Conference on Tangible, Embedded, and Embodied Interactions},
  day = {18},
  doi = {10.1145/3173225.3173272},
  finishday = {18},
  finishmonth = {Mar},
  finishyear = {2018},
  keyword = {e-textile},
  publicationstatus = {accepted},
  startday = {21},
  startmonth = {Mar},
  startyear = {2018},
  timestamp = {2018.02.05}
}
</pre>

<a name="stockman2018perceptioncues"></a><pre>
@article{<a href="pubs2018_raw.html#stockman2018perceptioncues">stockman2018perceptioncues</a>,
  title = {Perception of objects that move in depth, using ecologically valid audio cues},
  author = {STOCKMAN, T and Wilkie, S},
  journal = {Applied Acoustics},
  year = {2018},
  month = {Jan},
  day = {6},
  issn = {1872-910X},
  publicationstatus = {published},
  publisher = {Elsevier},
  timestamp = {2018.02.05}
}
</pre>

<hr><p><em>This file was generated by
<a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.98.</em></p>
